# -*- coding: utf-8 -*-
"""Iv_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j8ux94X2-OBPlInENbUNJwhHUgFUMjSc

# Invistico_Airlines Classification

## Methods Used-

 1. Logistic Regression
 2. KNN algorithm
 3. Decision Tree
 4. Support Vector Machine

### Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import itertools
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import NullFormatter
import pandas as pd
import numpy as np
import matplotlib.ticker as ticker
from sklearn import preprocessing
import seaborn as sns
# %matplotlib inline

#pip install -U imbalanced-learn --user

"""# Read Dataset"""

iv = pd.read_csv(r'C:\Users\ASUS\Desktop\Invistico_Airline.csv')

iv.head(5)

iv.columns

iv.shape

iv['satisfaction'].value_counts()

iv['Customer Type'].value_counts()

sns.set_style('whitegrid')
fig=sns.countplot(x='satisfaction', hue='Gender', data=iv, palette='GnBu_d')
for i in fig.patches:
    # get_x pulls left or right; get_height pushes up or down
    fig.text(i.get_x()+.07, i.get_height()+1000, str(round((i.get_height()), 2)), fontsize=12, color='black',
                rotation=0)

sns.set_style('whitegrid')
fig=sns.countplot(x='Customer Type', hue='Gender', data=iv, palette='BuGn_r')

for i in fig.patches:
    # get_x pulls left or right; get_height pushes up or down
    fig.text(i.get_x()+.07, i.get_height()+1000, str(round((i.get_height()), 2)), fontsize=12, color='black',
                rotation=0)

sns.set_style('whitegrid')
fig=sns.countplot(x='Class', hue='Gender', data=iv,palette='PiYG_r')
for i in fig.patches:
    # get_x pulls left or right; get_height pushes up or down
    fig.text(i.get_x()+.07, i.get_height()+1000, str(round((i.get_height()), 2)), fontsize=12, color='black',
                rotation=0)

iv.describe()

"""###  Detect Null Values"""

iv.isnull().sum()

"""### Delete Rows with Null Values"""

iv=iv.dropna()

iv.shape

iv = iv.reset_index(drop=True)

iv.head(-5)

"""# Dummy Creation

##  Satisfaction-------> 
  
  ### * Satisfied = 1
  ### * Dissatisfied = 0
  
## Gender--------->
 
 ### * Female = 1
 ### * Male = 0
 
## Customer Type------->
 
 ### * Loyal Customer = 1
 ### * Disloyal Customer = 0
 
## Type of Travel--------->
 
 ### * Business Travel = 1
 ### * Personal Travel = 0
 
## Class------------->
 
 ### * Business = reference Class
 ### * Eco
 ### * Eco Plus
"""

iv['satisfaction'].value_counts()

iv['satisfaction'].replace(to_replace=['satisfied','dissatisfied'], value=[1,0],inplace=True)

iv['Gender'].value_counts()

iv['Gender'].replace(to_replace=['Female','Male'], value=[1,0],inplace=True)

iv['Customer Type'].value_counts()

iv['Customer Type'].replace(to_replace=['Loyal Customer','disloyal Customer'], value=[1,0],inplace=True)

iv['Type of Travel'].value_counts()

iv['Type of Travel'].replace(to_replace=['Business travel','Personal Travel'], value=[1,0],inplace=True)

Class = pd.get_dummies(iv['Class'],prefix='class',drop_first=True)

Class.head()

iv = pd.concat([iv, Class], axis=1)

iv=iv.drop(['Class'], axis=1)

"""# Feature Selection"""

iv.columns

feature_cols=['Gender', 'Customer Type', 'Age', 'Type of Travel',
       'Flight Distance', 'Seat comfort', 'Departure/Arrival time convenient',
       'Food and drink', 'Gate location', 'Inflight wifi service',
       'Inflight entertainment', 'Online support', 'Ease of Online booking',
       'On-board service', 'Leg room service', 'Baggage handling',
       'Checkin service', 'Cleanliness', 'Online boarding',
       'Departure Delay in Minutes', 'Arrival Delay in Minutes', 'class_Eco',
       'class_Eco Plus']
dept_col=['satisfaction']
X=iv[feature_cols]
y=iv[dept_col]

str_list = [] # empty list to contain columns with strings (words)
for colname, colvalue in iv.iteritems():
    if type(colvalue[1]) == str:
         str_list.append(colname)
# Get to the numeric columns by inversion            
num_list = iv.columns.difference(str_list) 
# Create Dataframe containing only numerical features
house_num = iv[num_list]
f, ax = plt.subplots(figsize=(16, 12))
plt.title('Pearson Correlation of features')
# Draw the heatmap using seaborn
#sns.heatmap(house_num.astype(float).corr(),linewidths=0.25,vmax=1.0, square=True, cmap="PuBuGn", linecolor='k', annot=True)
sns.heatmap(house_num.astype(float).corr(),linewidths=0.25,vmax=1.0, square=True, cmap="cubehelix", linecolor='k', annot=True)

"""## SMOTE"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix,accuracy_score

from imblearn.over_sampling import SMOTE
os = SMOTE(random_state=0)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)
columns = X_train.columns
os_data_X,os_data_y=os.fit_sample(X_train, y_train)
os_data_X = pd.DataFrame(data=os_data_X,columns=columns )
os_data_y= pd.DataFrame(data=os_data_y,columns=['satisfaction'])
# we can Check the numbers of our data
print("length of oversampled data is ",len(os_data_X))
print("Number of dissatisfaction in oversampled data",len(os_data_y[os_data_y['satisfaction']==0]))
print("Number of satisfaction",len(os_data_y[os_data_y['satisfaction']==1]))
print("Proportion of dissatisfaction data in oversampled data is ",len(os_data_y[os_data_y['satisfaction']==0])/len(os_data_X))
print("Proportion of satisfaction data in oversampled data is ",len(os_data_y[os_data_y['satisfaction']==1])/len(os_data_X))

feature_cols=['Gender', 'Customer Type', 'Age', 'Type of Travel',
       'Flight Distance', 'Seat comfort', 'Departure/Arrival time convenient',
       'Food and drink', 'Gate location', 'Inflight wifi service',
       'Inflight entertainment', 'Online support', 'Ease of Online booking',
       'On-board service', 'Leg room service', 'Baggage handling',
       'Checkin service', 'Cleanliness', 'Online boarding',
       'Departure Delay in Minutes', 'Arrival Delay in Minutes', 'class_Eco',
       'class_Eco Plus']

X=os_data_X[feature_cols]
y=os_data_y['satisfaction']

"""# ***********   Top Factors Analysis

## Methods Used

### Recursive Factor Exclusion , Linear Regression , Ridge , Lasso , Random Forest
"""

from sklearn.feature_selection import RFE, f_regression
from sklearn.linear_model import (LinearRegression, Ridge, Lasso)
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor

# Define dictionary to store our rankings
ranks = {}
# Create our function which stores the feature rankings to the ranks dictionary
def ranking(ranks, names, order=1):
    minmax = MinMaxScaler()
    ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]
    ranks = map(lambda x: round(x,2), ranks)
    return dict(zip(names, ranks))

# Construct our Linear Regression model
lr = LinearRegression(normalize=True)
lr.fit(X,y)
#stop the search when only the last feature is left
rfe = RFE(lr, n_features_to_select=1, verbose =3 )
rfe.fit(X,y)
ranks["RFE"] = ranking(list(map(float, rfe.ranking_)), feature_cols, order=-1)

# Using Linear Regression
lr = LinearRegression(normalize=True)
lr.fit(X,y)
ranks["LinReg"] = ranking(np.abs(lr.coef_), feature_cols)

# Using Ridge 
ridge = Ridge(alpha = 7)
ridge.fit(X,y)
ranks['Ridge'] = ranking(np.abs(ridge.coef_), feature_cols)

# Using Lasso
lasso = Lasso(alpha=.05)
lasso.fit(X, y)
ranks["Lasso"] = ranking(np.abs(lasso.coef_), feature_cols)

rf = RandomForestRegressor(n_jobs=-1, n_estimators=50, verbose=3)
rf.fit(X,y)
ranks["RF"] = ranking(rf.feature_importances_, feature_cols);

# Create empty dictionary to store the mean value calculated from all the scores
r = {}
for name in feature_cols:
    r[name] = round(np.mean([ranks[method][name] 
                             for method in ranks.keys()]), 2)
 
methods = sorted(ranks.keys())
ranks["Mean"] = r
methods.append("Mean")
 
print("\t                %s" % "\t".join(methods))
for name in feature_cols:
    print("%s        \t       %s" % (name, "\t".join(map(str, 
                         [ranks[method][name] for method in methods]))))

# Put the mean scores into a Pandas dataframe
meanplot = pd.DataFrame(list(r.items()), columns= ['Feature','Mean Ranking'])

# Sort the dataframe
meanplot = meanplot.sort_values('Mean Ranking', ascending=False)

# Let's plot the ranking of the features
sns.factorplot(x="Mean Ranking", y="Feature", data = meanplot, kind="bar", 
               size=17, aspect=1.9, palette='coolwarm')

"""## Factors Exclusion

### RFE
"""

X.columns

iv_vars=iv.columns.values.tolist()
y=['satisfaction']
X=[i for i in iv_vars if i not in y]
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression(random_state=1, solver='newton-cg',
                         multi_class='multinomial',max_iter=1200)
rfe = RFE(logreg,20)
rfe = rfe.fit(os_data_X, os_data_y.values.ravel())
print(rfe.support_)
print(rfe.ranking_)

feature_cols=['Gender', 'Customer Type', 'Age', 'Type of Travel',
       'Flight Distance', 'Seat comfort', 'Departure/Arrival time convenient',
       'Food and drink', 'Gate location', 'Inflight wifi service',
       'Inflight entertainment', 'Online support', 'Ease of Online booking',
       'On-board service', 'Leg room service', 'Baggage handling',
       'Checkin service', 'Cleanliness', 'Online boarding',
       'Departure Delay in Minutes', 'Arrival Delay in Minutes', 'class_Eco',
       'class_Eco Plus']

X=os_data_X[feature_cols]
y=os_data_y['satisfaction']

import statsmodels.api as sm
logit_model=sm.Logit(y,X)
result=logit_model.fit()
print(result.summary2())

"""### Arrival delay , Departure delay , Flight distance is removed from features due to their low contribution (not significant)"""

feature_cols=['Gender', 'Customer Type', 'Age', 'Type of Travel', 'Seat comfort', 'Departure/Arrival time convenient',
       'Food and drink', 'Gate location', 'Inflight wifi service',
       'Inflight entertainment', 'Online support', 'Ease of Online booking',
       'On-board service', 'Leg room service', 'Baggage handling',
       'Checkin service', 'Cleanliness',
        'class_Eco',
       'class_Eco Plus']

X=os_data_X[feature_cols]
y=os_data_y['satisfaction']

import statsmodels.api as sm
logit_model=sm.Logit(y,X)
result=logit_model.fit()
print(result.summary2())

"""# Logistic R"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix,accuracy_score

"""### split x and y into training and testing sets"""

X= preprocessing.StandardScaler().fit(X).transform(X)

x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.25,random_state=1)

# instantiate the model (using the default parameters)
logreg = LogisticRegression(random_state=1, solver='lbfgs',
                         multi_class='multinomial',max_iter=1200000)

# fit the model with data
logreg.fit(x_train,y_train)

#
y_pred=logreg.predict(x_test)

print(logreg.coef_)
print(logreg.intercept_)

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)
cnf_matrix

class_names=[0,1] # name  of classes
fig, ax = plt.subplots()
tick_marks = np.arange(len(class_names))
plt.xticks(tick_marks, class_names)
plt.yticks(tick_marks, class_names)
# create heatmap
sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="viridis" ,fmt='g')
ax.xaxis.set_label_position("top")
plt.tight_layout()
plt.title('Confusion matrix', y=1.1)
plt.ylabel('Actual label')
plt.xlabel('Predicted label')

print(classification_report(y_test, y_pred))

"""## Accuracy & precision"""

print("Accuracy:",metrics.accuracy_score(y_test, y_pred))
print("Precision:",metrics.precision_score(y_test, y_pred))
print("Recall:",metrics.recall_score(y_test, y_pred))

"""## ROC Curve"""

y_pred_proba = logreg.predict_proba(x_test)[::,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label="data 1, auc="+str(auc))
plt.legend(loc=4)
plt.show()

print("Area under curve : ", auc)

"""# K Neighbour Classifier"""

from sklearn.metrics import jaccard_similarity_score
from sklearn.metrics import jaccard_score
from sklearn.metrics import f1_score
from sklearn.metrics import log_loss
from sklearn.model_selection import cross_val_score

trainScores={}

from sklearn.neighbors import KNeighborsClassifier

bestScore=0.0
accList=[]

for k in range(3,12):
    
    clf_knn = KNeighborsClassifier(n_neighbors=k,algorithm='auto')
    
    # using 10 fold cross validation for scoring the classifier's accuracy
    scores = cross_val_score(clf_knn, x_train, y_train, cv=10)
    score=scores.mean()
    accList.append(score)
    
    if score > bestScore:
        bestScore=score
        best_clf=clf_knn
        bestK=k
        
print("Best K is :",bestK,"| Cross validation Accuracy :",bestScore)
clf_knn=best_clf

plt.plot(range(3,12),accList)
plt.xlabel('K')
plt.ylabel('CV Accuracy')
plt.show()

clf_knn.fit(x_train,y_train)
y_pred=best_clf.predict(x_train)

trainScores['KNN-jaccard']=jaccard_score(y_train, y_pred)
trainScores['KNN-f1-score']=f1_score(y_train, y_pred, average='weighted')

trainScores

"""# Decision Tree"""

from sklearn import tree

clf_tree = tree.DecisionTreeClassifier()
clf_tree = clf_tree.fit(x_train, y_train)

y_pred=clf_tree.predict(x_train)

trainScores['Tree-jaccard']=jaccard_score(y_train, y_pred)
trainScores['Tree-f1-score']=f1_score(y_train, y_pred, average='weighted')

trainScores

"""##  *******   Decision tree graph (Use if needed, heavy lag)  ***************

!pip install graphviz
!pip install pydotplus
import graphviz 
import pydotplus

dot_data = tree.export_graphviz(clf_tree, out_file=None, 
                     feature_names=['Gender', 'Customer Type', 'Age', 'Type of Travel', 'Flight Distance',
       'Seat comfort', 'Departure/Arrival time convenient', 'Food and drink',
       'Gate location', 'Inflight wifi service', 'Inflight entertainment',
       'Online support', 'Ease of Online booking', 'On-board service',
       'Leg room service', 'Baggage handling', 'Checkin service',
       'Cleanliness', 'Online boarding', 'Departure Delay in Minutes',
       'Arrival Delay in Minutes', 'class_Eco', 'class_Eco Plus'],  
                     class_names='satisfaction',  
                     filled=True, rounded=True,  
                     special_characters=True) 

graph = pydotplus.graph_from_dot_data(dot_data)
graph.set_size('"16,16!"')
gvz_graph = graphviz.Source(graph.to_string())

gvz_graph

# Support vector Machine (SVM)
"""

y_train=y_train.astype(float)

from sklearn import svm

clf_svm = svm.LinearSVC(random_state=7,max_iter=120000)
clf_svm.fit(x_train, y_train)  

y_pred=clf_svm.predict(x_train)

trainScores['SVM-jaccard']=jaccard_score(y_train, y_pred)
trainScores['SVM-f1-score']=f1_score(y_train, y_pred, average='weighted')

trainScores

"""# LogReg 2"""

from sklearn.linear_model import LogisticRegression

clf_log = LogisticRegression(random_state=1, solver='newton-cg',
                         multi_class='multinomial')
clf_log.fit(x_train, y_train)

y_pred=clf_log.predict(x_train)
y_proba=clf_log.predict_proba(x_train)

trainScores['LogReg-jaccard']=jaccard_score(y_train, y_pred)
trainScores['LogReg-f1-score']=f1_score(y_train, y_pred, average='weighted')  
trainScores['LogReg-logLoss']=log_loss(y_train, y_proba)

trainScores

"""# Test Scores"""

testScores={}

"""### Knn test"""

knn_pred=clf_knn.predict(x_test)
testScores['KNN-jaccard']=jaccard_score(y_test, knn_pred)
testScores['KNN-f1-score']=f1_score(y_test, knn_pred, average='weighted')

"""### Decision Tree Test"""

tree_pred=clf_tree.predict(x_test)
testScores['Tree-jaccard']=jaccard_score(y_test, tree_pred)
testScores['Tree-f1-score']=f1_score(y_test, tree_pred, average='weighted')

"""### SVM test"""

svm_pred=clf_svm.predict(x_test)
testScores['SVM-jaccard']=jaccard_score(y_test, svm_pred)
testScores['SVM-f1-score']=f1_score(y_test, svm_pred, average='weighted')

"""### LogReg Test"""

log_pred=clf_log.predict(x_test)
proba=clf_log.predict_proba(x_test)
testScores['LogReg-jaccard']=jaccard_score(y_test, log_pred)
testScores['LogReg-f1-score']=f1_score(y_test, log_pred, average='weighted')  
testScores['LogReg-logLoss']=log_loss(y_test, proba)

testScores

"""# Test Scores Comparison"""

Jaccard = [0.8603978300180832,0.8809438684304612,0.7290215588723051,0.7289608064195239]
F1_score = [0.9273326220056296,0.9373511471170297,0.8462708616556703,0.8461964801035043]
LogLoss = ['NA','NA','NA',0.3692225185807503]

    
df = {'Algorithm': ['KNN', 'Decision Tree', 'SVM', 'LogisticRegression'], \
     'Jaccard': Jaccard, 'F1-score': F1_score, 'LogLoss': LogLoss}

Report = pd.DataFrame(data=df, columns=['Algorithm', 'Jaccard', 'F1-score', 'LogLoss'], index=None)
Report

